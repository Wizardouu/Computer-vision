{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9455528,"sourceType":"datasetVersion","datasetId":5747912}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport keras\n\ndataset_path = os.listdir('/kaggle/input/shop-dataset/Shop DataSet')\n\nlabel_types = os.listdir('/kaggle/input/shop-dataset/Shop DataSet')\nprint (label_types) ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:22.436689Z","iopub.execute_input":"2024-09-29T12:05:22.436989Z","iopub.status.idle":"2024-09-29T12:05:34.517050Z","shell.execute_reply.started":"2024-09-29T12:05:22.436955Z","shell.execute_reply":"2024-09-29T12:05:34.515967Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['non shop lifters', 'shop lifters']\n","output_type":"stream"}]},{"cell_type":"code","source":"rooms = []\n\nfor item in dataset_path:\n # Get all the file names\n all_rooms = os.listdir('/kaggle/input/shop-dataset/Shop DataSet' + '/' +item)\n\n # Add them to the list\n for room in all_rooms:\n    rooms.append((item, str('/kaggle/input/shop-dataset/Shop DataSet' + '/' +item) + '/' + room))\n    \n# Build a dataframe        \ndataSet_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\nprint(dataSet_df.head())\nprint(dataSet_df.tail())","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:36.693095Z","iopub.execute_input":"2024-09-29T12:05:36.694240Z","iopub.status.idle":"2024-09-29T12:05:37.143590Z","shell.execute_reply.started":"2024-09-29T12:05:36.694198Z","shell.execute_reply":"2024-09-29T12:05:37.142646Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"                tag                                         video_name\n0  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n1  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n2  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n3  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n4  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n              tag                                         video_name\n850  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n851  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n852  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n853  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n854  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n","output_type":"stream"}]},{"cell_type":"code","source":"df = dataSet_df.loc[:,['video_name','tag']]\ndf\ndf.to_csv('dataSet.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:37.755008Z","iopub.execute_input":"2024-09-29T12:05:37.755403Z","iopub.status.idle":"2024-09-29T12:05:37.774085Z","shell.execute_reply.started":"2024-09-29T12:05:37.755366Z","shell.execute_reply":"2024-09-29T12:05:37.773170Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataSet_df = pd.read_csv(\"dataSet.csv\")\nprint(f\"Total videos in dataSet: {len(dataSet_df)}\")\ndataSet_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:39.281512Z","iopub.execute_input":"2024-09-29T12:05:39.282273Z","iopub.status.idle":"2024-09-29T12:05:39.305182Z","shell.execute_reply.started":"2024-09-29T12:05:39.282234Z","shell.execute_reply":"2024-09-29T12:05:39.304228Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total videos in dataSet: 855\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                         video_name  \\\n761         761  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n197         197  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n430         430  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n651         651  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n71           71  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n723         723  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n294         294  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n368         368  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n385         385  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n712         712  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n\n                  tag  \n761      shop lifters  \n197  non shop lifters  \n430  non shop lifters  \n651      shop lifters  \n71   non shop lifters  \n723      shop lifters  \n294  non shop lifters  \n368  non shop lifters  \n385  non shop lifters  \n712      shop lifters  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>video_name</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>761</th>\n      <td>761</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>197</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>430</th>\n      <td>430</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>651</th>\n      <td>651</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>71</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>723</th>\n      <td>723</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>294</th>\n      <td>294</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>368</th>\n      <td>368</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>385</th>\n      <td>385</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>712</th>\n      <td>712</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = dataSet_df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:39.894097Z","iopub.execute_input":"2024-09-29T12:05:39.894485Z","iopub.status.idle":"2024-09-29T12:05:39.899082Z","shell.execute_reply.started":"2024-09-29T12:05:39.894449Z","shell.execute_reply":"2024-09-29T12:05:39.898166Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef extract_and_sample_frames(video_path, num_frames=16):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate frame sampling interval\n    sample_interval = max(1, total_frames // num_frames)\n\n    count = 0\n    while len(frames) < num_frames and cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Sample frames at regular intervals\n        if count % sample_interval == 0:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame_rgb)\n\n        count += 1\n\n    # If fewer frames are extracted, pad with repeated frames\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n\n    cap.release()\n    return frames\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:40.955234Z","iopub.execute_input":"2024-09-29T12:05:40.955602Z","iopub.status.idle":"2024-09-29T12:05:41.150857Z","shell.execute_reply.started":"2024-09-29T12:05:40.955568Z","shell.execute_reply":"2024-09-29T12:05:41.150089Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nimport torch\n\n# Load pre-trained video transformer\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\nmodel = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\")\n\n# Preprocess frames\ndef preprocess_video_frames(video_path):\n    frames = extract_and_sample_frames(video_path, num_frames=16)\n    inputs = feature_extractor(frames, return_tensors=\"pt\")\n    return inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:05:45.561559Z","iopub.execute_input":"2024-09-29T12:05:45.561939Z","iopub.status.idle":"2024-09-29T12:05:58.980899Z","shell.execute_reply.started":"2024-09-29T12:05:45.561901Z","shell.execute_reply":"2024-09-29T12:05:58.979986Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24e99a18f2c4814bceff481cde16938"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720f9f2e466946b19afac76427977976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b540ada3ac4061b2c8e27850280d8e"}},"metadata":{}},{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress bar\nimport torch\n\nvideo_paths = df['video_name'].tolist()  # Replace with your video path column name\nlabels = df['tag'].tolist() \n\n# Define a mapping from string labels to numeric labels\nlabel_mapping = {\n    'shop lifters': 1,\n    'non shop lifters': 0\n}\n\n# Preprocess all videos\ninputs_list = []\nlabels_list = []\n\n# Use tqdm to wrap the zip iterator\nfor video_path, label in tqdm(zip(video_paths, labels), total=len(video_paths), desc=\"Processing videos\"):\n    inputs = preprocess_video_frames(video_path)\n    inputs_list.append(inputs['pixel_values'])  # Append preprocessed video\n\n    # Convert label to numeric form and append to the list\n    numeric_label = label_mapping[label]\n    labels_list.append(numeric_label)\n\n# Stack inputs and labels\ninputs_tensor = torch.cat(inputs_list)\nlabels_tensor = torch.tensor(labels_list)\n\nprint(f\"Inputs tensor shape: {inputs_tensor.shape}\")\nprint(f\"Labels tensor shape: {labels_tensor.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:06:59.529876Z","iopub.execute_input":"2024-09-29T12:06:59.530508Z","iopub.status.idle":"2024-09-29T12:21:02.598229Z","shell.execute_reply.started":"2024-09-29T12:06:59.530468Z","shell.execute_reply":"2024-09-29T12:21:02.597183Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Processing videos:   0%|          | 0/855 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.tensor(value)\nProcessing videos: 100%|██████████| 855/855 [14:00<00:00,  1.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Inputs tensor shape: torch.Size([855, 16, 3, 224, 224])\nLabels tensor shape: torch.Size([855])\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n# First, ensure tensors have the correct shapes\nprint(f\"Inputs tensor shape: {inputs_tensor.shape}\")\nprint(f\"Labels tensor shape: {labels_tensor.shape}\")\n\n# Create a dataset using TensorDataset\ndataset = TensorDataset(inputs_tensor, labels_tensor)\n\n# Define the split proportions\ntrain_ratio = 0.75\nvalid_ratio = 0.10\ntest_ratio = 0.15\n\n# Calculate the lengths of each split\ndataset_size = len(dataset)\ntrain_size = int(train_ratio * dataset_size)\nvalid_size = int(valid_ratio * dataset_size)\ntest_size = dataset_size - train_size - valid_size\n\n# Split the dataset into training, validation, and test sets\ntrain_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n\n# Create dataloaders for each split\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\nprint(f\"Train size: {train_size}, Valid size: {valid_size}, Test size: {test_size}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:22:02.940620Z","iopub.execute_input":"2024-09-29T12:22:02.941290Z","iopub.status.idle":"2024-09-29T12:22:03.390336Z","shell.execute_reply.started":"2024-09-29T12:22:02.941249Z","shell.execute_reply":"2024-09-29T12:22:03.389420Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Inputs tensor shape: torch.Size([855, 16, 3, 224, 224])\nLabels tensor shape: torch.Size([855])\nTrain size: 641, Valid size: 85, Test size: 129\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move model to the GPU\nmodel = model.to(device)\n\n# Create a dataset and dataloader\ntrain_dataset = TensorDataset(inputs_tensor, labels_tensor)  # Assuming you've already split your data\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Assuming you have a validation dataset\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n\n# Define optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Function to calculate accuracy\ndef calculate_accuracy(preds, labels):\n    _, predicted_classes = torch.max(preds, 1)\n    correct_predictions = (predicted_classes == labels).sum().item()\n    return correct_predictions / labels.size(0)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    total_train_acc = 0\n\n    # Training\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n        inputs, labels = batch\n        \n        # Move inputs and labels to the GPU\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(pixel_values=inputs)\n        loss = loss_fn(outputs.logits, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss and accuracy\n        total_train_loss += loss.item()\n        total_train_acc += calculate_accuracy(outputs.logits, labels)\n    \n    # Calculate average loss and accuracy for the epoch\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_train_acc = total_train_acc / len(train_loader)\n\n    # Validation\n    model.eval()\n    total_valid_loss = 0\n    total_valid_acc = 0\n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n            inputs, labels = batch\n            \n            # Move inputs and labels to the GPU\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(pixel_values=inputs)\n            loss = loss_fn(outputs.logits, labels)\n\n            # Accumulate validation loss and accuracy\n            total_valid_loss += loss.item()\n            total_valid_acc += calculate_accuracy(outputs.logits, labels)\n\n    # Calculate average validation loss and accuracy for the epoch\n    avg_valid_loss = total_valid_loss / len(valid_loader)\n    avg_valid_acc = total_valid_acc / len(valid_loader)\n\n    # Print the results for the current epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_acc:.4f}\")\n    print(f\"Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {avg_valid_acc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:22:06.258851Z","iopub.execute_input":"2024-09-29T12:22:06.259641Z","iopub.status.idle":"2024-09-29T12:39:46.113445Z","shell.execute_reply.started":"2024-09-29T12:22:06.259597Z","shell.execute_reply":"2024-09-29T12:39:46.112513Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/5: 100%|██████████| 107/107 [03:26<00:00,  1.93s/it]\nValidation Epoch 1/5: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\nTraining Loss: 0.2805, Training Accuracy: 0.8563\nValidation Loss: 0.0732, Validation Accuracy: 0.9318\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/5: 100%|██████████| 107/107 [03:26<00:00,  1.93s/it]\nValidation Epoch 2/5: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\nTraining Loss: 0.0590, Training Accuracy: 0.9755\nValidation Loss: 0.0063, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/5: 100%|██████████| 107/107 [03:26<00:00,  1.93s/it]\nValidation Epoch 3/5: 100%|██████████| 11/11 [00:05<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\nTraining Loss: 0.1061, Training Accuracy: 0.9673\nValidation Loss: 0.0121, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/5: 100%|██████████| 107/107 [03:26<00:00,  1.93s/it]\nValidation Epoch 4/5: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\nTraining Loss: 0.0168, Training Accuracy: 0.9942\nValidation Loss: 0.0016, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/5: 100%|██████████| 107/107 [03:26<00:00,  1.93s/it]\nValidation Epoch 5/5: 100%|██████████| 11/11 [00:05<00:00,  1.97it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\nTraining Loss: 0.0011, Training Accuracy: 1.0000\nValidation Loss: 0.0003, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\n# Define a mapping from the model's output to labels\nlabel_mapping = {1: 'Shoplifter', 0: 'Non-Shoplifter'}\n\n# Preprocessing function (same as used in training)\ndef preprocess_external_video(video_path):\n    # Assuming you have a function 'extract_and_sample_frames' to extract video frames\n    frames = extract_and_sample_frames(video_path, num_frames=16)  # Adjust the number of frames as needed\n    inputs = feature_extractor(frames, return_tensors=\"pt\")\n    return inputs['pixel_values']\n\n# Prediction function\ndef predict_shoplifter(video_path, model, device):\n    # Preprocess the external video\n    pixel_values = preprocess_external_video(video_path)\n\n    # Move the input to the device (GPU/CPU)\n    pixel_values = pixel_values.to(device)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Run the model with no gradient calculation (for inference)\n    with torch.no_grad():\n        # Make prediction\n        outputs = model(pixel_values=pixel_values)\n        logits = outputs.logits\n\n    # Get the predicted class (0: Non-Shoplifter, 1: Shoplifter)\n    predicted_class = torch.argmax(logits, dim=1).item()\n\n    # Map the predicted class to the actual label\n    prediction_label = label_mapping[predicted_class]\n\n    return prediction_label\n\n# Example usage\nexternal_video_path = \"/kaggle/input/shop-dataset/Shop DataSet/shop lifters/shop_lifter_0.mp4\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Make a prediction for an external video\nprediction = predict_shoplifter(external_video_path, model, device)\nprint(f\"The video is predicted to show: {prediction}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:40:12.105880Z","iopub.execute_input":"2024-09-29T12:40:12.106276Z","iopub.status.idle":"2024-09-29T12:40:13.157760Z","shell.execute_reply.started":"2024-09-29T12:40:12.106239Z","shell.execute_reply":"2024-09-29T12:40:13.156815Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"The video is predicted to show: Shoplifter\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport numpy as np\n\n# Function to evaluate the model on validation/test data and collect predictions and true labels\ndef evaluate_model(dataloader, model, device):\n    model.eval()  # Set the model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient calculation for evaluation\n        for batch in dataloader:\n            inputs, labels = batch\n\n            # Move inputs and labels to the GPU/CPU\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Forward pass to get model predictions\n            outputs = model(pixel_values=inputs)\n            logits = outputs.logits\n\n            # Get predicted class labels\n            preds = torch.argmax(logits, dim=1)\n\n            # Store the predictions and true labels\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\n# Function to print confusion matrix and classification report\ndef print_evaluation_metrics(dataloader, model, device):\n    # Get predictions and true labels\n    y_pred, y_true = evaluate_model(dataloader, model, device)\n\n    # Print confusion matrix\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n\n    # Print classification report (precision, recall, F1-score)\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['Non-Shoplifter', 'Shoplifter']))\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming you have a DataLoader for your validation or test data\nprint_evaluation_metrics(test_loader, model, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:41:17.438883Z","iopub.execute_input":"2024-09-29T12:41:17.439600Z","iopub.status.idle":"2024-09-29T12:41:25.874001Z","shell.execute_reply.started":"2024-09-29T12:41:17.439558Z","shell.execute_reply":"2024-09-29T12:41:25.872974Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[80  0]\n [ 0 49]]\n\nClassification Report:\n                precision    recall  f1-score   support\n\nNon-Shoplifter       1.00      1.00      1.00        80\n    Shoplifter       1.00      1.00      1.00        49\n\n      accuracy                           1.00       129\n     macro avg       1.00      1.00      1.00       129\n  weighted avg       1.00      1.00      1.00       129\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}