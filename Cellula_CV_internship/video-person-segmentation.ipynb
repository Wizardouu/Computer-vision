{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9455528,"sourceType":"datasetVersion","datasetId":5747912}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\n\n# Load pre-trained YOLOv5 model (for person detection)\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\nmodel.eval()\n\ndef detect_person_yolo(frame):\n    \"\"\"\n    Use YOLO to detect a person in the frame and return the bounding box.\n    \"\"\"\n    results = model(frame)\n    # Extract the bounding boxes for persons (class id 0 is for 'person' in COCO dataset)\n    person_boxes = [box for box in results.xyxy[0] if int(box[5]) == 0]\n    \n    if person_boxes:\n        # Take the first detected person\n        person_box = person_boxes[0].cpu().numpy()\n        x1, y1, x2, y2 = map(int, person_box[:4])\n        return (x1, y1, x2, y2), person_box[4]  # Returning bounding box and confidence\n    return None, None\n\ndef segment_and_color_person(frame, bbox):\n    \"\"\"\n    Highlight the detected person in the frame using a colored mask.\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n\n    # Create a mask for the person\n    mask = np.zeros_like(frame, dtype=np.uint8)\n    \n    # Set the mask to yellow in the bounding box area\n    mask[y1:y2, x1:x2] = (0, 255, 255)  # Yellow color (BGR format)\n\n    # Apply mask to the frame by blending the original frame and the mask\n    segmented_frame = cv2.addWeighted(frame, 1, mask, 0.5, 0)\n\n    return segmented_frame\n\ndef segment_and_save_video(video_path, output_path):\n    \"\"\"\n    Detect and segment persons in a video and save the processed video.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    # Define video writer for output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for output video\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Detect the person in the frame\n        bbox, confidence = detect_person_yolo(frame)\n        if bbox:\n            # Highlight the detected person with a colored mask\n            frame = segment_and_color_person(frame, bbox)\n            \n            # Draw bounding box around the person\n            x1, y1, x2, y2 = bbox\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green bounding box\n            \n            # Add confidence score text\n            cv2.putText(frame, f\"Person: {confidence:.2f}\", (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\n        # Write the modified frame (with the person segmented and colored) to the output video\n        out.write(frame)\n    \n    cap.release()\n    out.release()\n    print(f\"Processed video saved at: {output_path}\")\n\n# Example usage\nvideo_path = '/kaggle/input/shop-dataset/Shop DataSet/shop lifters/shop_lifter_10.mp4'  # Path to the input video\noutput_path = 'output_segmented_colored_video.mp4'  # Path to save the segmented video\nsegment_and_save_video(video_path, output_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}